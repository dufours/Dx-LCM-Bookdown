---
title: "Bayesian latent class models for prevalence estimation and diagnostic test evaluation"
author: "Simon Dufour and Juan Carlos Arango Sabogal"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography:
- book.bib
- packages.bib
biblio-style: apalike
link-citations: yes
description: Chapitre 4
output: html_document
---


```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```


# Exercise 1 - Proportions
## Data
The data used for the following exercises came from a study aiming at estimating diagnostic accuracy of a milk pregnancy associated glycoprotein (**PAG**) ELISA and of transrectal ultrasonographic (**US**) exam when used for determining pregnancy status of individual dairy cows 28 to 45 days post-insemination. In that study, the new test under investigation was the PAG test and US was used for comparison, but was considered as an imperfect reference test.  
  
In the original study, data from 519 cows from 18 commercial dairy herds were used. For the following exercises, the dataset was modified so that we have data from 519 cows from 3 herds (i.e., the data from 16 herds were collapsed together so they appear to be from one large herd). Note that there are some cows with missing values for one of the tests, so we will not always work with 519 cows. The complete original publication can be found here: [Dufour et al., 2017](https://www.sciencedirect.com/science/article/pii/S016758771630527X?casa_token=jmHY9HiOdEYAAAAA:kGdiIujRzrAQjjFRbGqtUxIBalDGvXllp6ja9w4T2s6c7yPbgc0asnak79bJ5GWWXI8InhCg0sg).  
  
The dataset *Preg.xlsx* contains the results from the study. However, **for the exercises we will always simply used the cross-tabulated data and these will be already organized for you and presented at the beginning of each exercise**. The dataset is still provided so you can further explore additional comparisons. The list of variables in the *Preg.xlsx* dataset are described in the table below.  
  
**Table**. List of variables in the *Preg.xlsx* dataset.  

| **Variable** | **Description**                               | **Range**                      |
|--------------|-----------------------------------------------|--------------------------------|
| Obs          | An observation unique ID number               | 1 to 519                       |
| Herd         | Herd ID number                                | 1 to 3                         |
| Cow          | Cow ID number                                 | NA                             |
| T1_DOP       | Number of days since insemination at testing  | 28 to 45                       |
| PAG_DX       | Results from the PAG test                     | 0 (not pregnant); 1 (pregnant) |
| US_DX        | Results from the ultrasound test              | 0 (not pregnant); 1 (pregnant) |
| TRE_DX       | Results from the transrectal exam (TRE) test  | 0 (not pregnant); 1 (pregnant) |
  
## Questions
In the study, we had the following proportion of apparently pregnant cows (based on the US exam).  
  
**257 US+/497 exams**  
  
Open up the `R` project for Exercise 1 (i.e., the R project file named *Exercise 1.Rproj*).   
  
In the Question 1 folder, you will find a partially completed `R` script named *Question 1.R*. To answer the following questions, try to complete the missing parts (they are highlighted by a **#TO COMPLETE#** comment). We also provided complete R scripts, but try to work it out on your own first! 
  
**Start from the partially completed *Question 1.R* `R` script located in Question 1 folder.**
  
1. What would be the proportion of pregnant cows? Use a Bayesian approach to compute that proportion and a credible interval (**CI**). For this first computation, assume that you have no prior knowledge on the expected proportion of pregnant cows in a Canadian herd. Run three Markov chains with different sets of initial values. Look at the trace plot. Do you think convergence was achieved? Do you need a longer burn-in period? Are all 3 chains converging in the same space? Compute the effective sample size (**ESS**), do you feel that number of iterations was sufficient to describe the posterior distribution with enough precision? What about auto-correlation of the Markov chains, any issues there?  
  
2. If you were to compute a Frequentist estimate with 95% CI, would it differ a lot from your Bayesian estimates? Why? As a refresher, the formula for a Frequentist 95%CI for a proportion is below (where *P* is the actual observed proportion and *n* is the denominator for that proportion):  
  
$95CI=P \pm 1.96* \sqrt{\frac{P*(1-P)}{n}}$  
  
3. In the literature, you saw a recent study conducted on 30 Canadian herds and reporting an expected pregnancy prevalence of 42% with 97.5% percentile of 74%. What kind of distribution could you use to represent this information? Use these information as a pregnancy prevalence prior distribution. Are your results very different?  
  
4. When comparing PAG to TUS results for the whole dataset, you got the following 2x2 table.  
  
**Table.** Cross-classified results of the PAG and TUS tests.  
  
|           | **TUS+** | **TUS-** | **Total** |
|-----------|----------|----------|-----------|
| **PAG+**  | 255      | 21       | 276       |
| **PAG-**  | 2        | 219      | 221       |
| **Total** | 257      | 240      | 497       |
  
Assuming that TUS is a gold-standard test could you compute PAG sensitivity (**Se**) and specificity (**Sp**)? Use vague priors for PAG Se and Sp since it is the first ever study on this topic (i.e., you do not have any prior knowledge on these unknown parameters).  
  
## Answers
1. What would be the proportion of pregnant cows? Use a Bayesian approach to compute that proportion and a credible interval (**CI**). For this first computation, assume that you have no prior knowledge on the expected proportion of pregnant cows in a Canadian herd. Run three Markov chains with different sets of initial values. Look at the trace plot. Do you think convergence was achieved? Do you need a longer burn-in period? Are all 3 chains converging in the same space? Compute the effective sample size (**ESS**), do you feel that number of iterations was sufficient to describe the posterior distribution with enough precision? What about auto-correlation of the Markov chains, any issues there?  
  
**Answer:** I chose to run a model with 3 chains of 10,000 iterations each (11,000 iterations minus a burn-in of 1,000). I have obtained the following diagnostic plots:  
  
![Diagnostic plot](Figures\Ex1_Q1_Dx.png)
  
Convergence of the 3 chains was achieved, all three chains appear to be moving in the same space (see trace plot). The 1,000 iterations burn-in period is probably more than needed for this very simple problem. Autocorrelation plot is just perfect with correlation declining very rapidly close to zero at lag of 1! The effective sample size for the *Prev* parameter is >17,000 values. So, plenty of precision to report the median and 2.5th and 97.5th percentiles.        
  
The median pregnancy prevalence estimate (95% CI) was 51.7% (47.3, 56.1).  
  
2. If you were to compute a frequentist estimate with 95% CI, would it differ a lot from your Bayesian estimates? Why?   
  
**Answer:** IT should not differ much from the Bayesian median estimate and 95CI because these latter estimates were generated using vague priors. In such cases, Bayesian and Frequentist estimates should be quite similar. Actually, if we use the Frequentist formula for computing 95CI and the observed data (i.e., 257/497) we get:  
  
$P = 257/497 = 0.517$  
  
$95CI=0.517 \pm 1.96* \sqrt{\frac{0.517*(1-0.517)}{497}} = 0.517 \pm 0.044$  
  
Thus, we have a Frequentist estimated proportion of 51.7% with a Frequentist 95 CI of 47.3 to 56.1 (virtually unchanged compared to the Bayesian estimates).  
  
3. In the literature, you saw a recent study conducted on 30 Canadian herds and reporting an expected pregnancy prevalence of 42% with 97.5% percentile of 74%. What kind of distribution could you use to represent this information? Use these information as a pregnancy prevalence prior distribution. Are your results very different?  
  
**Answer:** A beta(4.2, 5.4) distribution would have a mode of 0.42 and a 97.5th percentile of 0.74. Using this information as prior, I get a prevalence of pregnancy of 51.6% (95 CI: 47.2, 55.8). Actually, see very little difference between the models using vague *vs.* informative priors. This is because this informative prior contains only the equivalent of 10 observations $(4.2+5.4)$. In the dataset, there are 497 cows. Therefore, the estimation process is still mainly driven by our dataset.  
    
4. When comparing PAG to TUS results for the whole dataset, you got the following 2x2 table. Assuming that TUS is a gold-standard test could you compute PAG sensitivity (**Se**) and specificity (**Sp**)? Use vague priors for PAG Se and Sp since it is the first ever study on this topic (i.e., you do not have any prior knowledge on these unknown parameters).  
  
**Answer:** A sensitivity or a specificity are, similarly to a prevalence, simple proportions. Sensitivity is simply the number of test positive among the number of true positive. We have data for these in the 2x2 table.  If we assume that *US* is a gold-standard test, than number of true positive cows is 257. And 255 of them tested positive to the *PAG* test. Similarly, the specificity is simply the number of test negative among the number of true negative. From the 2x2 table, the number of true negative cows was 240 and 219 of them tested negative to the *PAG* test.  The likelihood functions (one for each parameter in this case) linking the unknown parameters (*Se* and *Sp*) to these observed data would be:  
  
$Test+ \sim dbin(Se, True+)$  
  
$Test- \sim dbin(Sp, True-)$
  
Using vague beta(1, 1) priors on the *Se* and *Sp*, I got an estimated *Se* of 99.0% (95 CI: 97.3, 99.8) and a *Sp* of 91.0% (95CI: 87.0, 94.2). But be cautious with these numbers. We know very well that an *US* exam is not a perfect diagnostic test for pregnancy in dairy cows. With the current approach, we are attributing all disagreements between tests to a failure of the *PAG* test... But it could be the *US* test that was wrong in some instances. Moreover, animals for which the two tests agreed could still be misdiagnosed as pregnant or open, but by both tests (i.e., a failure of the *PAG* **AND** the *US* tests). We wil see in the next parts of the course how to account for the imperfections of both tests.
  


  
  

  

