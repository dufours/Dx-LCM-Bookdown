---
title: "Bayesian latent class models for prevalence estimation and diagnostic test evaluation"
author: "Simon Dufour and Juan Carlos Arango Sabogal"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography:
- book.bib
- packages.bib
biblio-style: apalike
link-citations: yes
description: Chapitre 14
output: html_document
---

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```


# Sensitivity analyses
An important step of any Bayesian analysis is to evaluate whether the results are strongly affected by the model's assumptions and/or the choice of priors.  
  
## Model's assumptions
Regarding the sensitivity of our results to the assumptions made, this could be assessed by comparing the results from the LCM where a given assumption was made *vs.* the ones from a LCM where the assumption was relaxed. Of course, **to fully "isolate" the effect of a given assumption, the same priors would have to be used in both LCM**. This is, however, not always possible since, sometimes, the "relaxed" LCM may require a larger number of informative priors to make it identifiable.  
  
In the end, providing the results from an "alternative" LCM could help the readers understand what was the influence of the model chosen on the presented results. As author, you would probably want to use the LCM that is the most "biologically relevant" as your main model, but present the results of these alternative models to illustrate how the choice of model may have impacted the results.  
  
Below are some of the main assumptions that you may want to evaluate.  
  
### Conditional independence between tests  
Even if two tests rely on very different biological processes, it would be a good practice to evaluate whether allowing for conditional dependence between tests would produce different results. You could do this using the model described in Chapter 5.  
  
### Constant test accuracy across populations  
To evaluate whether test's accuracy is constant across populations, the following options could be used:  
  
- Check if the accuracy vary as function of a characteristic of the tested individuals (e.g., age, breed, etc);  
- If a substantial number of populations were studied:  
  - We could run the model after removing one of the population and repeat this process for each population. If the results (e.g., the test's accuracy parameters) differ substantially for one of the model, this could indicate that the test accuracy is different in this population;  
  - We could allow for different accuracy in one or many of the populations. For instance, we may define *TestA* accuracy as *SeA-2* and *SpA-2* in one (or some) of the populations studied and as *SeA* and *SpA* for all other populations. If the *SeA* and *SeA-2* (or *SpA* and *SpA-2*) parameters differ substantially, this may, again, indicate that the test accuracy is different in this population.  
  
### Prevalence varies from one population to another  
This latter assumption is important for the [Hui and Walter (1980)](https://www.jstor.org/stable/2530508?seq=1) model and other similar models. One thing that can be done is to “eyeball” the initial tests' results and/or the estimated prevalence to confirm some differences between populations in, respectively, the apparent and/or true prevalence.  
  
Besides, [Gustafson et al. (2005)](https://projecteuclid.org/journals/statistical-science/volume-20/issue-2/On-Model-Expansion-Model-Contraction-Identifiability-and-Prior-Information/10.1214/088342305000000098.full) have reviewed the impacts of having two populations with very similar prevalence and the only impact was a lack of identifiability of the model. This latter issue would results in non-converging Markov chains for one or a few of the unknown parameters. If the Markov chains converged, then the assumption of varying prevalence between population is probably not a problem.      
  
## Choice of priors  
Whenever informative priors are used in a LCM, the sensitivity of the LCM to the choice of priors should be assessed. This can be done by running the same LCM, but with "perturbed" and usually "more diffuse" priors. [Johnson et al. (2019)](https://doi.org/10.1016/j.prevetmed.2019.01.010) recommended, for instance, to move the mode of the prior distribution of a given parameter by a few percentage-points (e.g., -15 percentage-points) and to increase the width of its prior distribution. Then, the results from the LCM using the scientific literature-derived priors can be compared to those of the LCM using perturbed priors to assess how sensitive to the choice of priors the results are. The amount and direction by which the mode will be moved and the level of increase of the width of the distribution will depend on the problem at hand.  

For instance, in the preceding sections we used the [Romano et al. (2006)](https://www.sciencedirect.com/science/article/pii/S0093691X06001543?via%3Dihub) results, to inform the priors for the Se and Sp of a US exam conducted between 28-45 days post-insemination:  
  -Se = 0.90 (95CI: 0.85, 0.95)  
  -Sp = 0.95 (95CI: 0.90, 0.97)  
  
We thus came up with these prior distributions for US' Se and sp.  
  
```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(epiR)
# US sensitivity ----------------------------------------------------------
# Sensitivity of US:  Mode=0.90, and we are 97.5% sure >0.85 
Se.US <- epi.betabuster(mode=0.90, conf=0.975, imsure="greater than", x=0.85)  
#plot the Se prior distribution
curve(dbeta(x, shape1=Se.US$shape1, shape2=Se.US$shape2), from=0, to=1, 
      main="Prior for ultrasound exam sensitivity", xlab = "Proportion", ylab = "Density")



```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# US specificity ----------------------------------------------------------
# Specificity of US:  Mode=0.95, and we are 97.5% sure >0.90 
Sp.US <- epi.betabuster(mode=0.95, conf=0.975, imsure="greater than", x=0.90)  
#plot the Sp prior distribution
curve(dbeta(x, shape1=Sp.US$shape1, shape2=Sp.US$shape2), from=0, to=1, 
      main="Prior for ultrasound exam specificity", xlab = "Proportion", ylab = "Density")
```
  
Thus, a possibility could be to move down the Se and Sp modes by 10 percentage points (i.e., mode for Se: 0.80 and mode for Sp: 0.85) and to increase the width of the distribution by moving the 2.5th percentiles by 30 percentage-points (Se 2.5th percentile: 0.55; Sp 2.5th percentile: 0.60). Thus:  
  
```{r, message=FALSE, warning=FALSE, echo=FALSE}
# US sensitivity ----------------------------------------------------------
# Sensitivity of US:  Mode=0.80, and we are 97.5% sure >0.55 
Se.US <- epi.betabuster(mode=0.80, conf=0.975, imsure="greater than", x=0.55)  
#plot the Se prior distribution
curve(dbeta(x, shape1=Se.US$shape1, shape2=Se.US$shape2), from=0, to=1, 
      main="Perturbed prior for ultrasound exam sensitivity", xlab = "Proportion", ylab = "Density")

```
  
```{r, message=FALSE, warning=FALSE, echo=FALSE}
# US specificity ----------------------------------------------------------
# Specificity of US:  Mode=0.85, and we are 97.5% sure >0.60 
Sp.US <- epi.betabuster(mode=0.85, conf=0.975, imsure="greater than", x=0.60)  
#plot the Sp prior distribution
curve(dbeta(x, shape1=Sp.US$shape1, shape2=Sp.US$shape2), from=0, to=1, 
      main="Perturbed prior for ultrasound exam specificity", xlab = "Proportion", ylab = "Density")
```

Then, the results from the LCM with the informative and perturbed priors could be compared.  
  
For instance, in exercise 3 we compared two conditionally independent diagnostic tests (PAG and US) in three populations (herd #1, herd #2, and herd #3). For that model, though the model is identifiable, we could use the [Romano et al. (2006)](https://www.sciencedirect.com/science/article/pii/S0093691X06001543?via%3Dihub) informative priors on US' Se and Sp. Then, we could run the same LCM, but with the perturbed US' Se and Sp priors suggested above. If we do that, we would obtain the following results:  

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, results=FALSE}

# Generate data and labels ------------------------------------------------
datalist <- list(Pop1=c(138,1,10,113),
                 Pop2=c(71, 0, 8, 69),
                 Pop3=c(46, 1, 3, 37)
                 )

n <- sapply(datalist, sum)
nPop1 <- n[1]
nPop2 <- n[2]
nPop3 <- n[3]

TestA <- "US"
TestB <- "PAG"


# Information for prior distributions -------------------------------------

library(epiR) 

# US sensitivity ----------------------------------------------------------
# Sensitivity of US:  Mode=0.90, and we are 97.5% sure >0.85 
Se.US <- epi.betabuster(mode=0.90, conf=0.975, imsure="greater than", x=0.85)  

# US specificity ----------------------------------------------------------
# Specificity of US:  Mode=0.95, and we are 97.5% sure >0.90 
Sp.US <- epi.betabuster(mode=0.95, conf=0.975, imsure="greater than", x=0.90)  

# Assign values generated -------------------------------------------------
Prev1.shapea <- 1         #a shape parameter for Prev in population 1    
Prev1.shapeb <- 1         #b shape parameter for Prev in population 1

Prev2.shapea <- 1         #a shape parameter for Prev in population 2    
Prev2.shapeb <- 1         #b shape parameter for Prev in population 2

Prev3.shapea <- 1         #a shape parameter for Prev in population 3    
Prev3.shapeb <- 1         #b shape parameter for Prev in population 3

Se.TestA.shapea <- Se.US$shape1     #a shape parameter for Se of TestA
Se.TestA.shapeb <- Se.US$shape2     #b shape parameter for Se of TestA
Sp.TestA.shapea <- Sp.US$shape1     #a shape parameter for Sp of TestA
Sp.TestA.shapeb <- Sp.US$shape2     #b shape parameter for Sp of TestA

#I could use vague priors for Se and Sp of PAG
Se.TestB.shapea <- 1     #a shape parameter for Se of TestB
Se.TestB.shapeb <- 1     #b shape parameter for Se of TestB
Sp.TestB.shapea <- 1     #a shape parameter for Sp of TestB
Sp.TestB.shapeb <- 1     #b shape parameter for Sp of TestB


# Create the JAGS text file -----------------------------------------------
#Create the JAGS text file
model_2tests_3pop_indep <- paste0("model{

#=== LIKELIHOOD	===#

  #=== POPULATION 1 ===#
  Pop1[1:4] ~ dmulti(p1[1:4], ",nPop1,")
  p1[1] <- Prev1*Se_", TestA, "*Se_", TestB, " + (1-Prev1)*(1-Sp_", TestA, ")*(1-Sp_", TestB, ")
  p1[2] <- Prev1*Se_", TestA, "*(1-Se_", TestB, ") + (1-Prev1)*(1-Sp_", TestA, ")*Sp_", TestB, "
  p1[3] <- Prev1*(1-Se_", TestA, ")*Se_", TestB, " + (1-Prev1)*Sp_", TestA, "*(1-Sp_", TestB, ")
  p1[4] <- Prev1*(1-Se_", TestA, ")*(1-Se_", TestB, ") + (1-Prev1)*Sp_", TestA, "*Sp_", TestB, "

  #=== POPULATION 2 ===#  
  Pop2[1:4] ~ dmulti(p2[1:4], ",nPop2,")
  p2[1] <- Prev2*Se_", TestA, "*Se_", TestB, " + (1-Prev2)*(1-Sp_", TestA, ")*(1-Sp_", TestB, ")
  p2[2] <- Prev2*Se_", TestA, "*(1-Se_", TestB, ") + (1-Prev2)*(1-Sp_", TestA, ")*Sp_", TestB, "
  p2[3] <- Prev2*(1-Se_", TestA, ")*Se_", TestB, " + (1-Prev2)*Sp_", TestA, "*(1-Sp_", TestB, ")
  p2[4] <- Prev2*(1-Se_", TestA, ")*(1-Se_", TestB, ") + (1-Prev2)*Sp_", TestA, "*Sp_", TestB, "

  #=== POPULATION 3 ===#  
  Pop3[1:4] ~ dmulti(p3[1:4], ",nPop3,")
  p3[1] <- Prev3*Se_", TestA, "*Se_", TestB, " + (1-Prev3)*(1-Sp_", TestA, ")*(1-Sp_", TestB, ")
  p3[2] <- Prev3*Se_", TestA, "*(1-Se_", TestB, ") + (1-Prev3)*(1-Sp_", TestA, ")*Sp_", TestB, "
  p3[3] <- Prev3*(1-Se_", TestA, ")*Se_", TestB, " + (1-Prev3)*Sp_", TestA, "*(1-Sp_", TestB, ")
  p3[4] <- Prev3*(1-Se_", TestA, ")*(1-Se_", TestB, ") + (1-Prev3)*Sp_", TestA, "*Sp_", TestB, "  
  
#=== PRIOR	===#

  Prev1 ~ dbeta(",Prev1.shapea,", ",Prev1.shapeb,") 	## Prior for Prevalence in population 1
  Prev2 ~ dbeta(",Prev2.shapea,", ",Prev2.shapeb,") 	## Prior for Prevalence in population 2
  Prev3 ~ dbeta(",Prev3.shapea,", ",Prev3.shapeb,") 	## Prior for Prevalence in population 3
  Se_", TestA, " ~ dbeta(",Se.TestA.shapea,", ",Se.TestA.shapeb,") 	## Prior for Se of Test A
  Sp_", TestA, " ~ dbeta(",Sp.TestA.shapea,", ",Sp.TestA.shapeb,") 	## Prior for Sp of Test A
  Se_", TestB, " ~ dbeta(",Se.TestB.shapea,", ",Se.TestB.shapeb,") 	## Prior for Se of Test B
  Sp_", TestB, " ~ dbeta(",Sp.TestB.shapea,", ",Sp.TestB.shapeb,") 	## Prior for Sp of Test B
  
}")

#write as a text (.txt) file
write.table(model_2tests_3pop_indep, 
            file="model_2tests_3pop_indep.txt", 
            quote=FALSE, 
            sep="", 
            row.names=FALSE,
            col.names=FALSE)


# Generate initial values -------------------------------------------------
inits <- list(list(Prev1=0.50,
                   Prev2=0.50,
                   Prev3=0.50,
                   Se_US=0.90,
                   Sp_US=0.90,
                   Se_PAG=0.90,
                   Sp_PAG=0.90),
              
              list(Prev1=0.60,
                   Prev2=0.60,
                   Prev3=0.60,
                   Se_US=0.80,
                   Sp_US=0.80,
                   Se_PAG=0.80,
                   Sp_PAG=0.80),
              
              list(Prev1=0.40,
                   Prev2=0.40,
                   Prev3=0.40,
                   Se_US=0.70,
                   Sp_US=0.70,
                   Se_PAG=0.70,
                   Sp_PAG=0.70)
              )

# Run the model -----------------------------------------------------------
library(R2jags)
library(coda)

#Run the Bayesian model
bug.out <- jags(data=datalist,                             
                model.file="model_2tests_3pop_indep.txt",     
                parameters.to.save=c("Prev1", "Prev2", "Prev3", "Se_US", "Sp_US", "Se_PAG", "Sp_PAG"),               
                n.chains=3,                                 
                inits=inits,                                
                n.iter=11000,                                
                n.burnin=1000,                              
                n.thin=1,                                   
                DIC=FALSE)  


# Check diagnostic plots --------------------------------------------------
library(mcmcplots)
bug.mcmc <- as.mcmc(bug.out)          
mcmcplot(bug.mcmc, title="Diagnostic plots") 


# Compute Effective sample size -------------------------------------------
effectiveSize(bug.mcmc)


# Check results -----------------------------------------------------------
print(bug.out, digits.summary=3) 
```
```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, results=FALSE}

# US sensitivity ----------------------------------------------------------
# Sensitivity of US:  Mode=0.80, and we are 97.5% sure >0.55 
Se.US <- epi.betabuster(mode=0.80, conf=0.975, imsure="greater than", x=0.55)  

# US specificity ----------------------------------------------------------
# Specificity of US:  Mode=0.85, and we are 97.5% sure >0.60 
Sp.US <- epi.betabuster(mode=0.85, conf=0.975, imsure="greater than", x=0.60)  

# Assign values generated -------------------------------------------------
Se.TestA.shapea <- Se.US$shape1     #a shape parameter for Se of TestA
Se.TestA.shapeb <- Se.US$shape2     #b shape parameter for Se of TestA
Sp.TestA.shapea <- Sp.US$shape1     #a shape parameter for Sp of TestA
Sp.TestA.shapeb <- Sp.US$shape2     #b shape parameter for Sp of TestA

# Create the JAGS text file -----------------------------------------------
#Create the JAGS text file
model_2tests_3pop_indep <- paste0("model{

#=== LIKELIHOOD	===#

  #=== POPULATION 1 ===#
  Pop1[1:4] ~ dmulti(p1[1:4], ",nPop1,")
  p1[1] <- Prev1*Se_", TestA, "*Se_", TestB, " + (1-Prev1)*(1-Sp_", TestA, ")*(1-Sp_", TestB, ")
  p1[2] <- Prev1*Se_", TestA, "*(1-Se_", TestB, ") + (1-Prev1)*(1-Sp_", TestA, ")*Sp_", TestB, "
  p1[3] <- Prev1*(1-Se_", TestA, ")*Se_", TestB, " + (1-Prev1)*Sp_", TestA, "*(1-Sp_", TestB, ")
  p1[4] <- Prev1*(1-Se_", TestA, ")*(1-Se_", TestB, ") + (1-Prev1)*Sp_", TestA, "*Sp_", TestB, "

  #=== POPULATION 2 ===#  
  Pop2[1:4] ~ dmulti(p2[1:4], ",nPop2,")
  p2[1] <- Prev2*Se_", TestA, "*Se_", TestB, " + (1-Prev2)*(1-Sp_", TestA, ")*(1-Sp_", TestB, ")
  p2[2] <- Prev2*Se_", TestA, "*(1-Se_", TestB, ") + (1-Prev2)*(1-Sp_", TestA, ")*Sp_", TestB, "
  p2[3] <- Prev2*(1-Se_", TestA, ")*Se_", TestB, " + (1-Prev2)*Sp_", TestA, "*(1-Sp_", TestB, ")
  p2[4] <- Prev2*(1-Se_", TestA, ")*(1-Se_", TestB, ") + (1-Prev2)*Sp_", TestA, "*Sp_", TestB, "

  #=== POPULATION 3 ===#  
  Pop3[1:4] ~ dmulti(p3[1:4], ",nPop3,")
  p3[1] <- Prev3*Se_", TestA, "*Se_", TestB, " + (1-Prev3)*(1-Sp_", TestA, ")*(1-Sp_", TestB, ")
  p3[2] <- Prev3*Se_", TestA, "*(1-Se_", TestB, ") + (1-Prev3)*(1-Sp_", TestA, ")*Sp_", TestB, "
  p3[3] <- Prev3*(1-Se_", TestA, ")*Se_", TestB, " + (1-Prev3)*Sp_", TestA, "*(1-Sp_", TestB, ")
  p3[4] <- Prev3*(1-Se_", TestA, ")*(1-Se_", TestB, ") + (1-Prev3)*Sp_", TestA, "*Sp_", TestB, "  
  
#=== PRIOR	===#

  Prev1 ~ dbeta(",Prev1.shapea,", ",Prev1.shapeb,") 	## Prior for Prevalence in population 1
  Prev2 ~ dbeta(",Prev2.shapea,", ",Prev2.shapeb,") 	## Prior for Prevalence in population 2
  Prev3 ~ dbeta(",Prev3.shapea,", ",Prev3.shapeb,") 	## Prior for Prevalence in population 3
  Se_", TestA, " ~ dbeta(",Se.TestA.shapea,", ",Se.TestA.shapeb,") 	## Prior for Se of Test A
  Sp_", TestA, " ~ dbeta(",Sp.TestA.shapea,", ",Sp.TestA.shapeb,") 	## Prior for Sp of Test A
  Se_", TestB, " ~ dbeta(",Se.TestB.shapea,", ",Se.TestB.shapeb,") 	## Prior for Se of Test B
  Sp_", TestB, " ~ dbeta(",Sp.TestB.shapea,", ",Sp.TestB.shapeb,") 	## Prior for Sp of Test B
  
}")

#write as a text (.txt) file
write.table(model_2tests_3pop_indep, 
            file="model_2tests_3pop_indep.txt", 
            quote=FALSE, 
            sep="", 
            row.names=FALSE,
            col.names=FALSE)



# Run the model -----------------------------------------------------------
library(R2jags)
library(coda)

#Run the Bayesian model
bug.out <- jags(data=datalist,                             
                model.file="model_2tests_3pop_indep.txt",     
                parameters.to.save=c("Prev1", "Prev2", "Prev3", "Se_US", "Sp_US", "Se_PAG", "Sp_PAG"),               
                n.chains=3,                                 
                inits=inits,                                
                n.iter=11000,                                
                n.burnin=1000,                              
                n.thin=1,                                   
                DIC=FALSE)  


# Check diagnostic plots --------------------------------------------------
library(mcmcplots)
bug.mcmc <- as.mcmc(bug.out)          
mcmcplot(bug.mcmc, title="Diagnostic plots") 


# Compute Effective sample size -------------------------------------------
effectiveSize(bug.mcmc)


# Check results -----------------------------------------------------------
print(bug.out, digits.summary=3) 
```

**Table.** Results from a 2 tests, 3 populations LCM using different priors (literature-based *vs.* perturbed informative priors on US' Se and Sp). The priors differing between LCM are indicated in bold.  
  
| **Parameter** | **Romano et al. 2006 priors** |                      |   | **Perturbed priors** |                      |
|:-------------:|:-----------------------------:|:--------------------:|:-:|:--------------------:|:--------------------:|
|               |           **Prior**           |     **Posterior**    |   |       **Prior**      |     **Posterior**    |
|   **Prev1**   |         Beta(1.0, 1.0)        | 0.558 (0.496, 0.618) |   |    Beta (1.0, 1.0)   | 0.556 (0.494, 0.617) |
|   **Prev2**   |         Beta(1.0, 1.0)        | 0.523 (0.440, 0.604) |   |    Beta (1.0, 1.0)   | 0.520 (0.435, 0.603) |
|   **Prev3**   |         Beta(1.0, 1.0)        | 0.559 (0.452, 0.662) |   |    Beta (1.0, 1.0)   | 0.557 (0.449, 0.657) |
|   **Se US**   |       **Beta (100, 12)**      | 0.925 (0.893, 0.954) |   |  **Beta (13.6, 4.1)** | 0.931 (0.890, 0.970) |
|   **Sp US**   |      **Beta (100, 6.2)**      | 0.977 (0.956, 0.990) |   |  **Beta (14.0, 3.3)** | 0.981 (0.957, 0.994) |
|   **Se PAG**  |        Beta (1.0, 1.0)        |  0.996 (0.981, 1.00) |   |    Beta (1.0, 1.0)   |  0.996 (0.980, 1.00) |
|   **Sp PAG**  |        Beta (1.0, 1.0)        | 0.981 (0.934, 0.999) |   |    Beta (1.0, 1.0)   | 0.977 (0.922, 0.999) |  
  
In this specific example, we can see that the results are virtually unchanged. Thus, we could say that our initial analysis was quite robust to the choice of prior distributions. Personally, we would present the LCM with the [Romano et al. (2006)](https://www.sciencedirect.com/science/article/pii/S0093691X06001543?via%3Dihub) priors as our "main" model. If we do have valid information from the literature about one or many of the unknown parameters, then why not using them? 
  



