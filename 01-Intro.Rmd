---
title: "Bayesian latent class models for prevalence estimation and diagnostic test evaluation"
author: "Simon Dufour and Juan Arango Sabogal"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography:
- book.bib
- packages.bib
biblio-style: apalike
link-citations: yes
description: Chapitre 2
output: html_document
---

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```


# Generating prior distributions
One of the first thing that you will need for any Bayesian analysis is a way to generate and visualize a prior distribution corresponding to some scientific knowledge that you have about an unknown parameter. Generally, you will use information such as the mean and variance or the mode and the 2.5th (or 97.5th) percentile to find a corresponding distribution. You will use various distributions. Here is a list of the most important ones, we will give more details on each of them in the following sections:  
  
**- Normal distribution:** defined by a mean (mu) and its standard deviation (SD) or variance (tau). In some packages and software, `rjags` and OpenBUGS for instance, the inverse of the variance (1/tau) is used to specify a given normal distribution.   
**Notation: dNorm(mu, 1/tau)**  
  
**- Uniform distribution:** defined by a minimum (min) and maximum (max).  
**Notation: dUnif(min, max)**     
  
**- Beta distribution:** bounded between 0 and 1, beta distributions are defined by two shape parameters (a) and (b).  
**Notation: dBeta(a, b)**    
  
**- Bernoulli distribution:** For variables that can take the value 0 or 1. Bernoulli distributions are defined very simply by a single probability, P, which is the probability that the variable takes the value 1.  
**Notation: dBern(P)**   
  
**- Binomial distribution:** Also for variables that can take the value 0 or 1. Binomial distributions are defined by a probability, P, which is the probability that the variable takes the value 1, and a number of "trials", n. Thus, binomial distributions can be used to describe the number of individuals that will have the outcome, based on an expected prevalence (P) and a number of tested individuals (n).  
**Notation: dBin(P, n)**   
  
**- Multinomial distribution:** made up of a number of probabilities, all bounded between 0 and 1, and that, together, will sum up to 1. Multinomial distributions are defined by k probabilities (P1, P2, ..., Pk) and a number of observation (n).  
**Notation: dmulti(P[1:k], n)**     
  
## Normal distribution
The `dnorm()` function can be used to generate a given Normal distribution and the `curve()` function can be used to visualize the generated distribution. These functions are already part of `R`, there is no need to upload a `R` package.  

```{r, message=FALSE, warning=FALSE, fig.cap="Density curve of a Normal distribution."}
curve(dnorm(x, mean=2.0, sd=0.5),                                    # I indicate mean and SD of the distribution
      from=-2, to=7,                                                 # I indicate limits for the plot
      main="Normal distribution with mean of 2.0 and SD of 0.5",     #Adding a title
      xlab = "Value", ylab = "Density")                              #Adding titles for axes

```
Note that a Normal distribution with mean of zero and a very large SD provides very little information. Such distribution would be referred to as a **uniform or flat distribution** (A.K.A.; a vague distribution).
```{r, message=FALSE, warning=FALSE, fig.cap="Density curve of a flat Normal distribution."}
curve(dnorm(x, mean=0.0, sd=10000000000),                                    
      from=-100, to=100,                                                 
      main="A flat Normal distribution",     
      xlab = "Value", ylab = "Density")                              

```
  
## Uniform distribution
In the same manner, we could visualize an uniform distribution using the `dunif()` function. In the following example, we assumed that any values between -5.0 and 5.0 are equally probable.
```{r, message=FALSE, warning=FALSE, fig.cap="Density curve of an Uniform distribution."}
curve(dunif(x, min=-5.0, max=5.0),                                    
      from=-10, to=10,                                                 
      main="Uniform distribution with -5.0 and 5.0 limits",    
      xlab = "Value", ylab = "Density")                              


```
  
## Beta distribution
Beta distributions are another type of distributions that will specifically be used for parameters that are proportions (i.e., bounded between 0.0 and 1.0). <b>For this specific workshop, they will be very handy, since a sensitivity, specificity, or a prevalence are all proportions </b>. The `epi.betabuster()` function from the `epiR` library can be used to define a given prior distribution based on previous knowledge. When you use the `epi.betabuster()` function, it creates a new `R` object containing different elements. Among these, one element will be named *shape1* and another *shape2*. These correspond to the a and b shape parameters of the corresponding Beta distribution.   
  
For instance we may know that the most likely value for the sensitivity of a given test is 0.85 and that we are 97.5% certain that it is greater than 0.75. With these values, we will be able to find the a and b shape parameters of the corresponding Beta distribution.
```{r, message=FALSE, warning=FALSE, fig.cap="Density curve of a Beta distribution for a test sensitivity."}
library(epiR) 

# Sensitivity of a test as Mode=0.85, and we are 97.5% sure >0.75 
rval <- epi.betabuster(mode=0.85, conf=0.975, greaterthan=T, x=0.75)  # I create a new object named rval

rval$shape1                #View the a shape parameter in rval
rval$shape2                ##View the b shape parameter in rval  

#plot the prior distribution
curve(dbeta(x, shape1=rval$shape1, shape2=rval$shape2), from=0, to=1, 
      main="Prior for test's sensitivity", xlab = "Proportion", ylab = "Density")
```
  
Note that a dBeta(1.0, 1.0) is a uniform beta distribution.
```{r, message=FALSE, warning=FALSE, fig.cap="Density curve of a Beta(1.0, 1.0) distribution."}
#plot the prior distribution
curve(dbeta(x, shape1=1.0, shape2=1.0), from=0, to=1, 
      main="A Beta(1.0, 1.0) or flat distribution", xlab = "Proportion", ylab = "Density")
```
  
## Bernoulli distribution
To complete  
  
## Binomial distribution  
To complete  
    
## Multinomial distribution
A last type of distribution that we will not address in great details here, but that we will use later in our LCM is the multinomial distribution. When an outcome is categorical with >2 categories, we could use a multinomial distribution to describe the probability that an individual has the value "A", or "B", or "C", etc. In this workshop, the main application of this distribution will be for describing the combined results of two (or more than two) diagnostic tests. For instance, if we cross-tabulate the results of Test A and Test B, we have four potential outcomes:  
  
-Test A+ and Test B+ (lets call n1 the number of individual in that cell)  
-Test A+ and Test B- (n2)  
-Test A- and Test B+ (n3)  
-Test A- and Test B- (n4) 
  
We can illustrate this as follow:  
  
```{r echo=FALSE, warning=FALSE, message=FALSE}
#Cr√©er une table
Results <- c("Test B+", "Test B-")
TestA_positive <- c("n1","n2")
TestA_negative <- c("n3","n4")
dp_data2 <- data.frame(Results, TestA_positive, TestA_negative)
```
```{r echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
library(kableExtra)
kable(dp_data2, col.names=c(" ", "Test A+", "Test A-"), caption="Cross-classified results from two diagnostic tests")%>%
  kable_styling()
```
  
Thus we could describe the probabilities of an individual falling into one of these cells of the 2x2 table as a multinomial distribution. In this specific case, we would say that the combined results of the 2 tests can be determined by the total number of individual tested (n) and the 4 probabilities:   
  
-P(Test A+ and Test B+) (let's call it P1)   
-P(Test A+ and Test B-) (P2)    
-P(Test A- and Test B+) (P3)   
-P(Test A- and Test B-) (P4)  
  
Thus n1 can be determined if we know P1 and n. This can be written, more generally, as:  
  
$n[1:4] ~ dmulti(P[1:4], n)$  

Which means: the values of n1 (or n2, n3, or n4) is determined by the probability of falling into the "Test A+ and Test B+" cell, which is P1 (or P2, P3, or P4 for the other cells), and by the total number of individuals tested (n). Nothing too surprising here... If I have a probability of 0.30 to fall in a given cell, and I have tested 100 individuals, I should find 30 individuals in that cell. Wow! Still, the multinomial distribution is nice because it will ensure that all our probabilities will sum up to exactly 1.0.    
  
